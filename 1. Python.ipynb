{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456180c1-a142-4460-af9b-9c8617046bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "x=1\n",
    "y=4\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7812a8d9-da1d-4a23-a264-53734da1491c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#Sum of the list\n",
    "list = [1,2,3,4,5]\n",
    "print(sum(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9b72be-505a-45e4-916c-405f6aa87bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "#Calculate average\n",
    "print(sum(list)/len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9358cf6-26af-4950-8a46-74b047d5353d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 30)\n"
     ]
    }
   ],
   "source": [
    "#tuple\n",
    "my_tuple = (10,20,30)\n",
    "print(my_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e55931-49f9-407f-aa96-0e4798676c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6}\n{1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "my_set = {1,2,3,4,5}\n",
    "my_set.add(6)\n",
    "print(my_set)\n",
    "my_set.update([2,7,8]) #add multiple values\n",
    "print(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8219ea-5b22-4413-af5b-0c206528f16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\ndict_keys(['name', 'age'])\ndict_values(['Alice', 26])\n"
     ]
    }
   ],
   "source": [
    "#Dictionary\n",
    "my_dict= {\"name\" : \"Alice\" , \"age\" : 25}\n",
    "print(my_dict[\"name\"]) # Output : Alice\n",
    "my_dict[\"age\"] = 26  # Updating value\n",
    "print(my_dict.keys())  #dict_keys(['name','age'])\n",
    "print(my_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82837a62-c3cd-4b0b-9b55-69cbf5f56dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1|Alice|\n|  2|  Bob|\n|  3|Carol|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PythonDF\").getOrCreate()\n",
    "\n",
    "#Creating DataFrame from list of tuples\n",
    "data =[(1,\"Alice\"),(2,\"Bob\"),(3,\"Carol\")]\n",
    "df = spark.createDataFrame(data,[\"id\",\"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5773409-0634-4455-9095-2d3f35a6cff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n|Alice|\n|  Bob|\n|Carol|\n+-----+\n\n+---+-----+\n| id| name|\n+---+-----+\n|  2|  Bob|\n|  3|Carol|\n+---+-----+\n\n3\n+-------+---+-----+\n|summary| id| name|\n+-------+---+-----+\n|  count|  3|    3|\n|   mean|2.0| null|\n| stddev|1.0| null|\n|    min|  1|Alice|\n|    max|  3|Carol|\n+-------+---+-----+\n\n+---+-----+-------+\n| id| name|country|\n+---+-----+-------+\n|  1|Alice|  India|\n|  2|  Bob|  India|\n|  3|Carol|  India|\n+---+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Filter rows\n",
    "df.filter(df[\"id\"] > 1).show()\n",
    "\n",
    "# Count rows\n",
    "print(df.count())\n",
    "\n",
    "# Describe summary statistics (numeric columns)\n",
    "df.describe().show()\n",
    "\n",
    "# Add new column with literal value\n",
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn(\"country\", lit(\"India\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b8d607-b4f1-4957-9913-1736a45cd213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+--------+-------+\n|      date|region|  product|quantity|revenue|\n+----------+------+---------+--------+-------+\n|2024-01-01| North|Product A|      10|  200.0|\n|2024-01-01| South|Product B|       5|  300.0|\n|2024-01-02| North|Product A|      20|  400.0|\n|2024-01-02| South|Product B|      10|  600.0|\n|2024-01-03|  East|Product C|      15|  375.0|\n+----------+------+---------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "sales_data = [\n",
    "    (\"2024-01-01\", \"North\", \"Product A\", 10, 200.0),\n",
    "    (\"2024-01-01\", \"South\", \"Product B\", 5 , 300.0),\n",
    "    (\"2024-01-02\", \"North\", \"Product A\", 20, 400.0),\n",
    "    (\"2024-01-02\", \"South\", \"Product B\", 10, 600.0),\n",
    "    (\"2024-01-03\", \"East\",  \"Product C\", 15, 375.0),\n",
    "]\n",
    "columns = [\"date\", \"region\", \"product\", \"quantity\", \"revenue\"]\n",
    "sales_df = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9798effd-0cf3-40c2-b13b-635a559f8971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n|  product|total_revenue|\n+---------+-------------+\n|Product A|        600.0|\n|Product B|        900.0|\n|Product C|        375.0|\n+---------+-------------+\n\n+------+--------+-------------+\n|region|Quantity|Total_revenue|\n+------+--------+-------------+\n| North|      30|        600.0|\n| South|      15|        900.0|\n|  East|      15|        375.0|\n+------+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Total quantity and revenue by region\n",
    "# • • groupBy0 is used to group data by a column.\n",
    "# • • agg0 performs aggregation such as sumO and avg0.\n",
    "# • • Aliases are used to rename columns for readability.\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Calculate total revenue per product\n",
    "total_revenue_df = sales_df.groupBy(\"product\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "total_revenue_df.show()\n",
    "\n",
    "\n",
    "#Total quantity and revenue by region\n",
    "# Calculate total revenue per region\n",
    "total_revenue_df = sales_df.groupBy(\"region\").agg(sum(\"quantity\").alias(\"Quantity\"),sum(\"revenue\").alias(\"Total_revenue\"))\n",
    "total_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5abac8e-0f7c-46ed-bd31-dda87958d98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|  product|avg_revenue|\n+---------+-----------+\n|Product A|       20.0|\n|Product B|       60.0|\n|Product C|       25.0|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Average Revenue per Product\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate average revenue per product\n",
    "avg_revenue_df = sales_df.groupBy(\"product\").agg((sum(\"revenue\")/sum(\"quantity\")).alias(\"avg_revenue\"))\n",
    "avg_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38c86d8-2060-4d74-9d10-9e1fdff10b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n|region|total_revenue|\n+------+-------------+\n| South|        900.0|\n+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Top-Perfom=rming Region (highest revenue)\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Calculate total revenue per region and get the region with the highest revenue\n",
    "top_region_df = (\n",
    "    sales_df.groupBy(\"region\")\n",
    "    .agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "    .orderBy(\"total_revenue\", ascending=False)\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "top_region_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69911a7c-589d-45f6-949d-b62d9895fb33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product</th><th>sum(revenue)</th></tr></thead><tbody><tr><td>Product A</td><td>600.0</td></tr><tr><td>Product B</td><td>900.0</td></tr><tr><td>Product C</td><td>375.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Product A",
         600.0
        ],
        [
         "Product B",
         900.0
        ],
        [
         "Product C",
         375.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sum(revenue)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Product name anf total revenue\n",
    "from pyspark.sql.functions import sum\n",
    "# sales.df.groupBy(\"product\").agg(sum(\"revenue\")).show()\n",
    "display(sales_df.groupBy(\"product\").agg(sum(\"revenue\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7e08a9-d2e6-4501-a28b-bbfeecfc6056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}